{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Out Neural Network Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_url = 'https://gist.github.com/NickyDCFP/d675b176350f6a1d54455ffc35e350f9/raw/' + \\\n",
    "          '220fdbc2aec82b8b3e33681e4465813c02b5fccf/Spotify_Youtube.csv'\n",
    "csv_df = pd.read_csv(csv_url)\n",
    "csv_df.rename(columns={'Unnamed: 0': 'ID'}, inplace=True)\n",
    "csv_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_columns = [\"Url_spotify\", \"Uri\", \"Url_youtube\", \"ID\", \"Artist\", \"Track\", \"Album\", \"Album_type\", \"Title\", \"Channel\", \"Description\", 'Licensed', 'official_video'] # For now, dropping all non-numeric columns\n",
    "df = csv_df.drop(irrelevant_columns, axis=1)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['Views', 'Likes', 'Comments']\n",
    "df = df.dropna()\n",
    "df = df.loc[df['Views'] != 0]\n",
    "num_likes = np.array(df['Likes']).astype(int)\n",
    "num_comments = np.array(df['Comments']).astype(int)\n",
    "num_views = np.array(df['Views']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df.drop(y_cols, axis=1)\n",
    "X = np.array(X_df).astype('float32')\n",
    "y = (num_likes / num_views).astype('float32') # Can also use comment proportion as target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "clf = IsolationForest()\n",
    "clf.fit(X_train)\n",
    "y_p = clf.predict(X_train)\n",
    "X_train = X_train[y_p == 1, :]\n",
    "y_train = y_train[y_p == 1]\n",
    "y_p_test = clf.predict(X_test)\n",
    "X_test = X_test[y_p_test == 1, :]\n",
    "y_test = y_test[y_p_test == 1]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    score = tf.py_function(r2_score, (y_true, y_pred), tf.float64)\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal 2 layer structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_2_layer():\n",
    "    for layer1 in [20, 17, 15, 13, 10]:\n",
    "        for layer2 in [15, 12, 8, 5]:\n",
    "            model = keras.Sequential([\n",
    "                            Dense(layer1, activation='sigmoid',input_shape=(X_train_scaled.shape[1], )),\n",
    "                            Dense(layer2, activation='sigmoid'),\n",
    "                            Dense(1)\n",
    "                        ])\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "            model.fit(X_train_scaled, y_train, epochs=150, batch_size=150, verbose=0)\n",
    "            loss, r_2 = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "            res = pd.concat([res, pd.DataFrame({\n",
    "            'Layer 1': layer1,\n",
    "            'Layer 2': layer2,\n",
    "            'R^2': r_2,\n",
    "            'Loss': loss\n",
    "        }, index=['Layer 1'])], ignore_index=True)\n",
    "    return res\n",
    "results = test_2_layer()\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Optimal 3 Layer NN Structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to fit and predict the model with given layers\n",
    "import matplotlib.pyplot as plt\n",
    "def fit_and_predict(activation='sigmoid', l1=0, l2=0, l3=0):\n",
    "    results = pd.DataFrame(columns=['Layer 1', 'Layer 2', 'Layer 3', 'R^2', 'Loss'])\n",
    "    layer1, layer2, layer3 = 20, 20, 5\n",
    "    for i in range(len((layer1, layer2, layer3))):\n",
    "        res = pd.DataFrame(columns=['Layer 1', 'Layer 2', 'Layer 3', 'R^2', 'Loss'])\n",
    "        for diff in range(0, -19, -1 + (-1 * i==2)):\n",
    "            trials = [0, 0]\n",
    "            for trial in range(3):\n",
    "                model = keras.Sequential([\n",
    "                            Dense(layer1 + (i==0) * diff, activation=activation,input_shape=(X_train_scaled.shape[1], )),\n",
    "                            Dense(layer2 + (i==1) * diff, activation=activation),\n",
    "                            Dense((10 + diff//2 )if (i==2) else layer3, activation=activation),\n",
    "                            Dense(1)\n",
    "                        ])\n",
    "                optimizer = Adam(learning_rate=0.001)\n",
    "                model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "                model.fit(X_train_scaled, y_train, epochs=150, batch_size=150, verbose=0)\n",
    "                loss, r_2 = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "                trials[0] += loss\n",
    "                trials[1] += r_2\n",
    "            loss, r_2 = trials[0]/3, trials[1]/3\n",
    "            res = pd.concat([res, pd.DataFrame({\n",
    "                'Layer 1': layer1 + (i==0) * diff,\n",
    "                'Layer 2': layer2 + (i==1) * diff,\n",
    "                'Layer 3': (10 + diff//2 )if (i==2) else layer3,\n",
    "                'R^2': r_2,\n",
    "                'Loss': loss\n",
    "            }, index=['Layer 1'])], ignore_index=True)\n",
    "        res = res.sort_values(by=f'Layer {i+1}', ascending=False)\n",
    "        # create a line plot with Layer 1 on the x-axis and R^2 on the y-axis\n",
    "        plt.plot(res[f'Layer {i+1}'], res['R^2'])\n",
    "        # set the axis labels and title\n",
    "        plt.xlabel(f'Layer {i+1}')\n",
    "        plt.ylabel('R^2')\n",
    "        plt.title(f'R^2 vs Layer {i+1}')\n",
    "        # display the plot\n",
    "        plt.show()\n",
    "        print(res.head())\n",
    "        # Sort the dataframe by R^2 value\n",
    "        results = pd.concat([results, res], axis=0)\n",
    "    results = results.sort_values(by='R^2', ascending=False)\n",
    "    return results\n",
    "\n",
    "results = fit_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Regularization for each layer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_regularization():\n",
    "    results = pd.DataFrame(columns=['Layer', 'L2', 'R^2', 'Loss'])\n",
    "    for layer in range(1,4):\n",
    "        res= pd.DataFrame(columns=['Layer', 'L2', 'R^2', 'Loss'])\n",
    "        l2 = 0.001\n",
    "        while(l2 <= 1000):\n",
    "            model = keras.Sequential([\n",
    "            Dense(20, activation='sigmoid', kernel_regularizer=regularizers.l2(l2 if layer == 1 else 0), input_shape=(X_train_scaled.shape[1], )),\n",
    "            Dense(20, activation='sigmoid', kernel_regularizer=regularizers.l2(l2 if layer == 2 else 0)),\n",
    "            Dense(5, activation='sigmoid', kernel_regularizer=regularizers.l2(l2 if layer == 3 else 0)),\n",
    "            Dense(1)\n",
    "        ]) \n",
    "            optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "            fit = model.fit(X_train_scaled, y_train,\n",
    "                        epochs=150,\n",
    "                        batch_size=150,\n",
    "                        verbose=0)\n",
    "            loss, r_2 = model.evaluate(X_test_scaled, y_test)\n",
    "            res = pd.concat([res, pd.DataFrame({\n",
    "                'Layer':layer,\n",
    "                'L2': l2, \n",
    "                'R^2': r_2, \n",
    "                'Loss': loss\n",
    "            }, index=['L2'])], ignore_index=True)\n",
    "            print(f'{loss}, {r_2}')\n",
    "            l2 *= 10\n",
    "        plt.plot(res[f'L2'], res['R^2'])\n",
    "        # set the axis labels and title\n",
    "        plt.xlabel(f'L2 Layer {layer}')\n",
    "        plt.ylabel('R^2')\n",
    "        plt.title(f'R^2 vs L2 Layer {layer}')\n",
    "        # display the plot\n",
    "        plt.show()\n",
    "        res = res.sort_values(by='R^2', ascending=False)\n",
    "        display(res.head())\n",
    "        results = pd.concat([results, res], axis=0)\n",
    "    results = results.sort_values(by='R^2', ascending=False)\n",
    "    return results\n",
    "results = test_regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing L2 regularization for each activation function and structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_structure(struct, activation, results):\n",
    "    res = pd.DataFrame(['Activation', 'Structure', 'Lambda', 'Train R^2', 'Train MSE', 'Test R^2', 'Test MSE'])\n",
    "    lambdas =  [0, 1e-7, 1e-6, 1e-3, 0.01, 0.1, 1, 2, 5, 10, 100, 1000]\n",
    "    for l2 in lambdas: \n",
    "        model = keras.Sequential([\n",
    "        Dense(struct[0], activation=activation, kernel_regularizer=regularizers.l2(l2), input_shape=(X_train_scaled.shape[1], )),\n",
    "        Dense(struct[1], activation=activation, kernel_regularizer=regularizers.l2(l2)),\n",
    "        Dense(struct[2], activation=activation, kernel_regularizer=regularizers.l2(l2)),\n",
    "        Dense(1)]) \n",
    "        optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "        fit = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=150,\n",
    "                    batch_size=150,\n",
    "                    verbose=0)\n",
    "        train_loss, train_r_2 = fit.history['loss'][-1], fit.history['r2'][-1]\n",
    "        test_loss, test_r_2 = model.evaluate(X_test_scaled, y_test)\n",
    "        res = pd.concat([res, pd.DataFrame({\n",
    "            'Activation': activation,\n",
    "            'Structure': \" \".join(str(x) for x in struct),\n",
    "            'Lambda': l2, \n",
    "            'Train R^2': train_r_2,\n",
    "            'Train MSE': train_loss,\n",
    "            'Test R^2': test_r_2,\n",
    "            'Test MSE': test_loss,\n",
    "        }, index=['Lambda'])], ignore_index=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(res['Lambda'], res['Train R^2'], label='Train', color='green')\n",
    "    ax.plot(res['Lambda'], res['Test R^2'], label='Test', color='red')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Lambda')\n",
    "    ax.set_ylabel('R^2')\n",
    "    ax.set_title(f'R^2 vs {activation} Activation with {\" \".join(str(x) for x in struct)} Structure')\n",
    "    plt.show()\n",
    "    results = pd.concat([results, res], axis=0)\n",
    "    return results\n",
    "\n",
    "def test_models(activations=['sigmoid','relu', 'tanh'], structures = ([20, 6, 5], [20, 20, 5], [20, 11, 5])):\n",
    "    results = pd.DataFrame(['Activation', 'Structure', 'Lambda', 'Train R^2', 'Train MSE', 'Test R^2', 'Test MSE'])\n",
    "    for struct in structures:\n",
    "        for activation in activations:\n",
    "            results = test_structure(struct, activation, results)\n",
    "            display(results.tail(5))\n",
    "    display(results)\n",
    "    return results\n",
    "\n",
    "results = test_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dropout Regularization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_structure(struct, activation, results):\n",
    "    res = pd.DataFrame(['Activation', 'Structure', 'Dropout', 'Train R^2', 'Train Loss', 'Test R^2', 'Test Loss'])\n",
    "    amts = [0.15, 0.25, 0.35, 0.5]\n",
    "    for amt in amts: \n",
    "        model = keras.Sequential([\n",
    "        Dense(struct[0], activation=activation, input_shape=(X_train_scaled.shape[1], )),\n",
    "        Dropout(amt),\n",
    "        Dense(struct[1], activation=activation),\n",
    "        Dropout(amt),\n",
    "        Dense(struct[2], activation=activation),\n",
    "        Dropout(amt),\n",
    "        Dense(1)]) \n",
    "        optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "        fit = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=150,\n",
    "                    batch_size=150,\n",
    "                    verbose=0)\n",
    "        train_loss, train_r_2 = fit.history['loss'][-1], fit.history['r2'][-1]\n",
    "        test_loss, test_r_2 = model.evaluate(X_test_scaled, y_test)\n",
    "        res = pd.concat([res, pd.DataFrame({\n",
    "            'Activation': activation,\n",
    "            'Structure': \" \".join(str(x) for x in struct),\n",
    "            'Dropout': amt, \n",
    "            'Train R^2': train_r_2,\n",
    "            'Train Loss': train_loss,\n",
    "            'Test R^2': test_r_2,\n",
    "            'Test Loss': test_loss,\n",
    "        }, index=['Dropout'])], ignore_index=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(res['Dropout'], res['Train R^2'], label='Train', color='green')\n",
    "    ax.plot(res['Dropout'], res['Test R^2'], label='Test', color='red')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Dropout')\n",
    "    ax.set_ylabel('R^2')\n",
    "    ax.set_title(f'R^2 vs {activation} Activation with {\" \".join(str(x) for x in struct)} Structure')\n",
    "    plt.show()\n",
    "    results = pd.concat([results, res], axis=0)\n",
    "    return results\n",
    "\n",
    "def test_models(activations=['sigmoid','relu', 'tanh'], structures = ([20, 6, 5], [20, 20, 5], [20, 11, 5])):\n",
    "    results = pd.DataFrame(['Activation', 'Structure', 'Dropout', 'Train R^2', 'Train Loss', 'Test R^2', 'Test Loss'])\n",
    "    for struct in structures:\n",
    "        for activation in activations:\n",
    "            results = test_structure(struct, activation, results)\n",
    "            display(results.tail(5))\n",
    "    display(results)\n",
    "    return results\n",
    "\n",
    "results = test_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values('Test R^2', ascending=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Transformations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = []\n",
    "for i in range(2, 4):\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "    transforms.append((X_train_poly, X_test_poly))\n",
    "\n",
    "res = pd.DataFrame(['Structure', 'Lambda', 'Train R^2', 'Train Loss', 'Test R^2', 'Test Loss', 'Degree'])\n",
    "for degree, transform in enumerate(transforms):\n",
    "    model = keras.Sequential([\n",
    "            Dense(20, activation='sigmoid', kernel_regularizer=regularizers.l2(1e-7), input_shape=(transform[0].shape[1], )),\n",
    "            Dense(6, activation='sigmoid', kernel_regularizer=regularizers.l2(1e-7)),\n",
    "            Dense(5, activation='sigmoid', kernel_regularizer=regularizers.l2(1e-7)),\n",
    "            Dense(1)])\n",
    "    optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "    fit = model.fit(transform[0], y_train,\n",
    "                epochs=150,\n",
    "                batch_size=150,\n",
    "                verbose=0)\n",
    "    train_loss, train_r_2 = fit.history['loss'][-1], fit.history['r2'][-1]\n",
    "    test_loss, test_r_2 = model.evaluate(transform[1], y_test)\n",
    "    res = pd.concat([res, pd.DataFrame({\n",
    "            'Structure': '20 6 5',\n",
    "            'Lambda': 1e-7,\n",
    "            'Train R^2': train_r_2,\n",
    "            'Train Loss': train_loss,\n",
    "            'Test R^2': test_r_2,\n",
    "            'Test Loss': test_loss,\n",
    "            'Degree': degree\n",
    "        }, index=['Structure'])], ignore_index=True)\n",
    "    model = keras.Sequential([\n",
    "                                Dense(20, activation='sigmoid',input_shape=(transform[0].shape[1], )),\n",
    "                                Dense(6, activation='sigmoid'),\n",
    "                                Dense(5, activation='sigmoid'),\n",
    "                                Dense(1)\n",
    "                            ])\n",
    "    optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "    fit = model.fit(transform[0], y_train,\n",
    "                    epochs=150,\n",
    "                    batch_size=150,\n",
    "                    verbose=0)\n",
    "    train_loss, train_r_2 = fit.history['loss'][-1], fit.history['r2'][-1]\n",
    "    test_loss, test_r_2 = model.evaluate(transform[1], y_test)\n",
    "    res = pd.concat([res, pd.DataFrame({\n",
    "            'Structure': '20 6 5',\n",
    "            'Lambda': 0,\n",
    "            'Train R^2': train_r_2,\n",
    "            'Train Loss': train_loss,\n",
    "            'Test R^2': test_r_2,\n",
    "            'Test Loss': test_loss,\n",
    "            'Degree': degree\n",
    "        }, index=['Structure'])], ignore_index=True)\n",
    "    model = keras.Sequential([\n",
    "                                Dense(20, activation='sigmoid',input_shape=(transform[0].shape[1], )),\n",
    "                                Dense(20, activation='sigmoid'),\n",
    "                                Dense(5, activation='sigmoid'),\n",
    "                                Dense(1)\n",
    "                            ])\n",
    "    optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "    fit = model.fit(transform[0], y_train,\n",
    "                    epochs=150,\n",
    "                    batch_size=150,\n",
    "                    verbose=0)\n",
    "    train_loss, train_r_2 = fit.history['loss'][-1], fit.history['r2'][-1]\n",
    "    test_loss, test_r_2 = model.evaluate(transform[1], y_test)\n",
    "    res = pd.concat([res, pd.DataFrame({\n",
    "            'Structure': '20 20 5',\n",
    "            'Lambda': 0,\n",
    "            'Train R^2': train_r_2,\n",
    "            'Train Loss': train_loss,\n",
    "            'Test R^2': test_r_2,\n",
    "            'Test Loss': test_loss,\n",
    "            'Degree': degree\n",
    "        }, index=['Structure'])], ignore_index=True)\n",
    "    model = keras.Sequential([\n",
    "                                Dense(20, activation='sigmoid',input_shape=(transform[0].shape[1], )),\n",
    "                                Dense(11, activation='sigmoid'),\n",
    "                                Dense(5, activation='sigmoid'),\n",
    "                                Dense(1)\n",
    "                            ])\n",
    "    optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "    fit = model.fit(transform[0], y_train,\n",
    "                    epochs=150,\n",
    "                    batch_size=150,\n",
    "                    verbose=0)\n",
    "    train_loss, train_r_2 = fit.history['loss'][-1], fit.history['r2'][-1]\n",
    "    test_loss, test_r_2 = model.evaluate(transform[1], y_test)\n",
    "    res = pd.concat([res, pd.DataFrame({\n",
    "            'Structure': '20 11 5',\n",
    "            'Lambda': 0,\n",
    "            'Train R^2': train_r_2,\n",
    "            'Train Loss': train_loss,\n",
    "            'Test R^2': test_r_2,\n",
    "            'Test Loss': test_loss,\n",
    "            'Degree': degree\n",
    "        }, index=['Structure'])], ignore_index=True)\n",
    "\n",
    "    display(res.tail())\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = res[res['Degree'] != 'NaN'].sort_values(by='Test R^2', ascending=False)\n",
    "results['Degree'] += 2\n",
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See relationship between test size and R^2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_test_size(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    l1_lambda = 0.1\n",
    "    l2_lambda = 0.1\n",
    "    model = keras.Sequential([\n",
    "        Dense(5, activation='sigmoid', kernel_regularizer=regularizers.l1(l1_lambda), input_shape=(X_train_scaled.shape[1], )),\n",
    "        Dense(5, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        Dropout(0.5),\n",
    "        Dense(1)\n",
    "    ]) \n",
    "    optimizer = Adam(learning_rate=0.0001) # Tried out 0.01 learning rate, not much different. 0.001 is the default. Lowering learning rate gives steady descent, nice\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r2])\n",
    "    fit = model.fit(X_train_scaled, y_train,\n",
    "                epochs=150,\n",
    "                batch_size=150,\n",
    "                verbose=0)\n",
    "    loss, r_2 = model.evaluate(X_test_scaled, y_test)\n",
    "    history = fit.history\n",
    "    history['loss'] = history['loss'][-1]\n",
    "    history['r2'] = history['r2'][-1]\n",
    "    history['test_size'] = test_size\n",
    "    return history, {'test_size': test_size, 'loss': loss, 'r2': r_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['test_size', 'loss', 'r2']\n",
    "training_error = pd.DataFrame(columns=cols)\n",
    "test_error = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(1, 50, 1):\n",
    "    test_size = i / 100\n",
    "    history, eval = try_test_size(X, y, test_size)\n",
    "    training_error = pd.concat([training_error, pd.DataFrame(history, index=['test_size'])], ignore_index=True)\n",
    "    test_error = pd.concat([test_error, pd.DataFrame(eval, index=['test_size'])], ignore_index=True)\n",
    "\n",
    "display(training_error.head())\n",
    "display(test_error.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_error['test_size'], training_error['loss'], label='Training MSE')\n",
    "plt.plot(test_error['test_size'], test_error['loss'], label='Test MSE')\n",
    "plt.title('Neural Network MSE Across Different Test Sizes')\n",
    "plt.xlabel('Test Size')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_error['test_size'], training_error['r2'], label='Training R2')\n",
    "plt.plot(test_error['test_size'], test_error['r2'], label='Test R2')\n",
    "plt.title('Neural Network R2 Across Different Test Sizes')\n",
    "plt.xlabel('Test Size')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
